---
title: "climodr"
output: 
  rmarkdown::html_vignette:
    toc: TRUE
    toc_depth: 3
    number_sections: TRUE
    highlight: tango
vignette: >
  %\VignetteIndexEntry{climodr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
Version 0.0.0.9000

Welcome to climate modeller in R, short climodr. 
This packages uses point data from climate stations, spectral imagery and elevation models to automatically create ready-to-use climate maps and statistics. This Vignette should guide you through the package, explain its functions and give you an idea of how to use climodr.

# Getting Started with climodr

The Idea of climodr is, to speed up climate modelling processes and make them easier to use. The package foresees that one needs to store relevant input data into one folder structure and the package does the rest. However its also possible to split up and do the automated steps manually to adjust settings or to get into detail in certain steps. 

*more cool stuff to follow*

## Downloading climodr

To start with climodr, you first need to download and install the package from the Environmental Informatics Lab (envima) @ Marburg University from Github. To do so, you need [devtools](https://www.r-project.org/nosvn/pandoc/devtools.html) installed to your R. Once devtools is installed, you can simply add climodr by following commands.   
  
Note: It may asks you to install all packages climodr needs to execute all its functions. Please install all of those.  

```{r install}
#install climodr
#devtools::install_github("envima/climodr")

#devtools::load_all()
library(climodr)
```

## How to setup climodr

Setting up climodr just requires one step, before you can get started.  
With the *envi.create()* function, one just points out a path, where the package should store all its data. 

```{r setup env}
#setting up the environment for climodr

envi.create("E:/climodr/vignette",
            memfrac = 0.8)
```

Climodr then creates an environment with three main folders:\
- Input (for all necessary data the user must bring)\
- Output (for ready-to-use data created by climodr)\
- Workflow (for climodr to store data during the process)\

The Input-Directory is the place, where all data, which shall be used for modelling, should be saved beforehand. It consists of four different folders:\
- dep (Dependency, like a resolution image or metadata)\
- raster (Raster data, work in progress)\
- tabular (Tabular data, containing climate data from the climate stations)\
- vector (Vector data, like the study area or climate station point data)\

See [list of possible inputs](link) for further details, what kind of input-data can be used.\
The Output-Folder is the place, where all final data, which is created by the package, is stored in. It consists of three different folders:\
- maps (basic ready-to-use maps)\
- predictions (plain prediction imagery)\
- statistics (perfomance of the predictions and other statistics)\

The Output-Directory contains all the reade-to-use data in some basic formats, which should be publication-ready if no other needs are wanted or required. 

The Workflow-Directory contains all steps in between the Input and the Output. In here there are models, test and training data, clean tabular data, and so on.

Note: Do not delete any of these folders, since climodr requires those to run properly!

# Pre-Processing

For this package, a small showcase product has been edited, which gets delivered with climodr. Its a small scene located south of the bale mountains in ethiopia. There are three climate stations located in this scene.\

Note: Raster preperation and index calculation is not included in the package yet. So one has to put a complete raster stack with all indices into the `workflow/wraster` folder.\

## Prepare tabular data for processing

First, we have to prepare the tabular data for further uses. The prep.csv function cleans up and removes all NA values from the data.
```{r prep csv}
prep.csv(method = "proc", safe_output = TRUE)

#check the created csv files
csv_files <- grep("_no_NAs.csv$", 
                  list.files(envrmt$path_tworkflow), 
                  value=TRUE)
csv_files
```
## Process tabular data to monthly means

Next, the data needs to be aggregated for monthly means.\

Note: In future the user will be able to choose the time period. 
```{r proc csv}
csv_data <- proc.csv(method = "monthly",
                              rbind = TRUE,
                              safe_output = TRUE)
head(csv_data)
```

## Spatial aggregation of tabular data

Next, the stations have to be located in space, with a coordinate system. This step is crucial to process the data in a modelation use case.\

Note: For some reason the coordinates slide between SWDR_200 and P_RT_NRT. This is no problem, but the order is still weird. Should be adressed in future.
```{r spat csv}
csv_spat <- spat.csv(method = "monthly",
                     des_file = "plot_description.csv",
                     safe_output = TRUE)
head(csv_spat)
```

## Pre-Process Raster Data for data extraction

```{r crop all}
crop.all(method = "MB_Timeseries", overwrite = TRUE)
```

```{r calc indices}
calc.indices(vi = "all",
             bands = c("blue", "green", "red", 
                       "nir", "nirb", 
                       "re1", "re2", "re3", 
                       "swir1", "swir2"),
             overwrite = TRUE)
```


## Finalize tabular data for modelling

Next, one needs to extract the data at the stations from the aerial imagery.
```{r finalize csv}
csv_fin <- fin.csv(method = "monthly",
                   safe_output = TRUE)
head(csv_fin)
```

# Processing

## Test for Autocorrelation

Next, one tests the data for autocorrelation. The evaluation vector contains all columns from the final table which should be tested for autocorrelation. It creates the first outputs in the package with one tabular-file per response-sensor, which contains all columns which should be excluded from the modulation because they autocorrelate. It also creates some visualization for the user of the autocorrelation.\

Note: A few columns are masked out, because they cause errors. Needs to be address in future.\
Note': The visualization is quite messy, when there are a lot of predictors. Maybe make it prettier in future.
```{r autocorr, warning = FALSE}
autocorr(
  method = "monthly",
  pred = 5, 
  resp = c(8:24),
  plot.corrplot = FALSE
  )
```

## Create climate models 



timespan = Vector with last two digits of years to build models from (in this example 2017 - 2021)\
climresp = Vector of rows to create models for. (In this example Ta_200, rH_200, SWDR_200, P_RT_NRT)\
classifier = Vector of all model variants to be used. In this case random forest = "rf", partial-least-squares = "pls", neural networks = "nnet" and linear regression = "lm"\
\
NOTE: This constellation already creates 64 models, this will need a lot of time\
\
seed = Number to "pick randomnes". With the seed one can reproduce random pulls.\
p = Number of how much in percentage should be splitted from the data into training data.\
fold = Charackter. Method to Create Space/Time folds. "LLO", "LTO" or "LLTO".\
mnote = Charackter. 6 digits, if one mannually alters the data, e.g. leaves out one climate station.\
predrows = Vector of rownumbers used as predictors.\
k = Number of unique temporal or spatial units.\
tc_method = Train control method. Default is cross validation "cv".\
metric = Summary Metric to select optimal model. Default: Root Mean Square Error "RMSE".\
autocorrelation = Logical Parameter. TRUE, if the results of the autocorrelation should be considered. 
doParallel = Logical Parameter. When set True, the Model-Process will parallelize on all cores except two, so your PC will slow down a lot. Only recommended for PCs with at least 8 Cores. Warning: Your PC wont be able to process other stuff efficiently during parallelization.\

Note: Loop doesn't work with more climresp'S; Cuts when looped into climresp = 9, runs if started mannually; climresp = 12 doesnt work; climresp = 15 doesnt work -> Eval vector gets overwritten, statistics dont work.
```{r model, eval = FALSE, warning = FALSE}
calc.model(
  method = "monthly",
  timespan = c(2017),
  climresp = c(5),
  classifier = c(
    "rf", 
    "pls", 
    "nnet", 
    "lm"
    ),
  seed = 707,
  p = 0.8,
  folds = "LLO",
  mnote = "vignette",
  predrows = c(8:24),
  tc_method = "cv",
  metric = "RMSE",
  autocorrelation = TRUE,
  doParallel = FALSE)
```

## Predictions

Further we can predict the scenes using the models with the `climpred` function. `climpred` also calculates an AOA.\ 

Note: There are no options yet, but it works. Because the evaluation data frame doesn't work right now, it predicts all models. In Future, it will only predict the models which perform best. 

```{r predict, eval = FALSE}
climpred(
  method = "monthly",
  mnote = "vignette", 
  AOA = FALSE)
```


Lets show the list of predictions:
```{r list predictions}
predlist <- list.files(envrmt$path_predictions, pattern = ".tif")
head(predlist)
```



```{r plot predictions}
climplot(
  mnote = "vignette",
  sensor = "Ta_200",
  aoa = FALSE,
  mapcolors = rev(heat.colors(50)),
  scale_position = "bottomleft",
  north_position = "topright"
)
```

## Validations

We can now validate the prediction with our test-dataset, that was created during the `calc.model`-function. This way, one can receive simple statistics like a cross table or a prediction summary.

Note: Validation doesn't work fully yet, because the evaluation data frame is still buggy. 
```{r validate, eval = FALSE}
validate()
```

# Plotting

To-Do:
..the code Seda made.. (Should be included as function for plotting the modelled results as maps)\
- quick map style (nice plot, and good color scale)\
- first start for the users costumized\
- tidyterra package ?\
