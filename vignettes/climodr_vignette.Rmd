---
title: "A short introduction to climodr"
output: 
  rmarkdown::html_vignette:
    toc: TRUE
    toc_depth: 3
    number_sections: TRUE
    highlight: tango
vignette: >
  %\VignetteIndexEntry{climodr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
Version 0.0.0.9000

Welcome to climate modeller in R, short climodr. 
This packages uses point data from climate stations, spectral imagery and 
elevation models to automatically create ready-to-use climate maps and 
statistics. This Vignette should guide you through the package, explain its 
functions and give you an idea of how to use climodr.

# Getting Started with climodr

The Idea of climodr is, to speed up climate modelling processes and make them 
easier to use. The package foresees that one needs to store relevant input data 
into one folder structure and the package does the rest using the example 
workflow provided in this vignette. 
The functions are still modifyable, so one can adjust the model workflow to your 
liking.

## Downloading climodr

To start with climodr, you first need to download and install the package 
from CRAN.  

```{r install, eval = FALSE}
#install climodr
install.packages("climodr")
```

It may asks you to install all packages climodr needs to execute all its 
functions. Its mandatory to install these packages, otherwise climodr won't be
able to execute its functions, as it takes many functions into its comprehensive
workflow.

You can also install the latest development version from the
Environmental Informatics Lab (envima) @ Marburg University from Github. 
To do so, you need [devtools](https://www.r-project.org/nosvn/pandoc/devtools.html)
installed to your R. Once devtools is installed, you can simply add climodr by 
following commands.  

```{r install}
#install climodr
# ---- GITHUB-LINK HIER EINFÃœGEN -----
devtools::load_all() # Solange Github-Repo noch privat
```

Nice to add:
Link to the help-pages on CRAN, once package is published

## How to setup climodr

Setting up climodr just requires one step, before you can get started.  
With the *envi.create()* function, one points out a path where the package 
should store all its data. There is also the *memfrac* argument. This argument 
allows you to change the fraction of your RAM the terra-package is allowed to use.
By default this number is pretty low, so this way the process can be sped up.

```{r setup env}
library(climodr)

#setting up the environment for climodr
envi.create("E:/climodr/vignette",
            memfrac = 0.8)
```

Climodr then creates an environment with three main folders:\
- Input (for all necessary data the user must bring)\
- Output (for ready-to-use data created by climodr)\
- Workflow (for climodr to store data during the process)\

The Input-Directory is the place, where all data, which shall be used for modelling, should be saved beforehand. It consists of four different folders:\
- dep (Dependency, like a resolution image or metadata)\
- raster (Raster data, work in progress)\
- tabular (Tabular data, containing climate data from the climate stations)\
- vector (Vector data, like the study area or climate station point data)\

See [list of possible inputs](link) for further details, what kind of input-data can be used.\
The Output-Folder is the place, where all final data, which is created by the package, is stored in. It consists of three different folders:\
- maps (basic ready-to-use maps)\
- predictions (plain prediction imagery)\
- statistics (perfomance of the predictions and other statistics)\

The Output-Directory contains all the reade-to-use data in some basic formats, which should be publication-ready if no other needs are wanted or required. 

The Workflow-Directory contains all steps in between the Input and the Output. In here there are models, test and training data, clean tabular data, and so on.

Note: 
 - Do not delete any of these folders, since climodr requires those to run properly!
 - The higher you set the fraction of RAM that climodr will use, the slower the
PC will become when running climodr, in case you want to do something in parallel 
on the PC. Using a fraction > 0.8 can even make it hard to use a browser while
using climodr.

# Pre-Processing

For this package, a small showcase product has been edited, which comes with 
climodr. Its a small scene located in Hainich national park in Germany. There are 
ten climate stations located in this scene.\

Note: This is just an example with very few stations and a very small scene,
which will not result into a good model and is only used for educational purposes. 

You don't have to use raw format data. If you have data, that equals one of the 
levels in this Pre-Processing step, you can step in at the corresponding stage.
For now you'll need to take care that the data matches the pattern climodr 
produces in this workflow. 

## Prepare tabular data for processing

First, we have to prepare the raw tabular data for further uses. The prep.csv 
function cleans up the data and removes all NA values from the data.

```{r prep csv}
prep.csv(method = "proc", safe_output = TRUE)

#check the created csv files
csv_files <- grep("_no_NAs.csv$", 
                  list.files(envrmt$path_tworkflow), 
                   value=TRUE)
csv_files
```

## Process tabular data to means

Next, the data needs to be aggregated for to the desired time steps.\
In this version one can aggregate data into "monthly" and into "anual" data.\
The *rbind* argument stores all climate station data into one file. This step is 
recomended, since the data usually will become way shorter after the time 
aggregation and is easier to be processed further this way. 


```{r proc csv}
csv_data <- proc.csv(method = "monthly",
                              rbind = TRUE,
                              safe_output = TRUE)
head(csv_data)
```

## Spatial aggregation of tabular data

Next, the stations have to be spacialy located in a coordinate system. This 
step is crucial to process the data in a modelation use case.\

```{r spat csv}
csv_spat <- spat.csv(method = "monthly",
                     des_file = "plot_description.csv",
                     safe_output = TRUE)
head(csv_spat)
```

## Pre-Process Raster Data for data extraction

Now, that we have spatial points of our stations, we can continue with our raster 
data. The prefered *method* here is "MB_Timeseries", which stands for 'Multiband 
Timeseries'. Use this method, if you provide multiple singleband rasters or raster 
stacks with different timestamps per scene. The functions sorts them by date and 
crops the data to our study area.

```{r crop all}
crop.all(method = "MB_Timeseries", overwrite = TRUE)
```

Next, we calculate some basic indices, so we can create more predictor variables
for our models. Therefore *vi* chooses the vegetation indices one wants to create.
You can either list the desired indices in a vector, or simply use "all" to 
generate all available indices. For more detailed information use *?calc.indices*
in the console.

```{r calc indices}
calc.indices(vi = "all",
             bands = c("blue", "green", "red", 
                       "nir", "nirb", 
                       "re1", "re2", "re3", 
                       "swir1", "swir2"),
             overwrite = TRUE)
```

## Finalize tabular data for modelling

Now, that we have spatial points as well as raster data, we can extract additional
predictor variables at the station points from the spatial raster data. Therefore 
we use the *fin.csv* function. The functions extracts the station points from the
spatial raster data and adds the data to our spatial points.

```{r finalize csv}
csv_fin <- fin.csv(method = "monthly",
                   safe_output = TRUE)
head(csv_fin)
```
Now the data is ready for further modelling.

# Processing

In this step the spatial raster data and the climate station data is ready to use. 
If your data isn't, check out the 'Pre-Processing' chapter.

## Test for Autocorrelation

First, one tests the data for autocorrelation. The evaluation vector contains 
all columns with the response and predictor variables, which will be tested for 
autocorrelation. It creates the first outputs in the package with one tabular-file 
per response-sensor, which contains all columns which should be excluded from 
the modulation because they autocorrelate. It also creates some visualization 
for the user of the autocorrelation, if *plot.corrplot* is set to *TRUE*.\

Note': The visualization is quite messy, when there are a lot of predictors. 
Maybe make it prettier in future.

```{r autocorr, warning = FALSE}
autocorr(
  method = "monthly",
  pred = 5, 
  resp = c(8:24),
  plot.corrplot = FALSE
  )
```

## Create climate models 



timespan = Vector with last two digits of years to build models from (in this example 2017 - 2021)\
climresp = Vector of rows to create models for. (In this example Ta_200, rH_200, SWDR_200, P_RT_NRT)\
classifier = Vector of all model variants to be used. In this case random forest = "rf", partial-least-squares = "pls", neural networks = "nnet" and linear regression = "lm"\
\
NOTE: This constellation already creates 64 models, this will need a lot of time\
\
seed = Number to "pick randomnes". With the seed one can reproduce random pulls.\
p = Number of how much in percentage should be splitted from the data into training data.\
fold = Charackter. Method to Create Space/Time folds. "LLO", "LTO" or "LLTO".\
mnote = Charackter. 6 digits, if one mannually alters the data, e.g. leaves out one climate station.\
predrows = Vector of rownumbers used as predictors.\
k = Number of unique temporal or spatial units.\
tc_method = Train control method. Default is cross validation "cv".\
metric = Summary Metric to select optimal model. Default: Root Mean Square Error "RMSE".\
autocorrelation = Logical Parameter. TRUE, if the results of the autocorrelation should be considered. 
doParallel = Logical Parameter. When set True, the Model-Process will parallelize on all cores except two, so your PC will slow down a lot. Only recommended for PCs with at least 8 Cores. Warning: Your PC wont be able to process other stuff efficiently during parallelization.\

Note: Loop doesn't work with more climresp'S; Cuts when looped into climresp = 9, runs if started mannually; climresp = 12 doesnt work; climresp = 15 doesnt work -> Eval vector gets overwritten, statistics dont work.
```{r model, eval = FALSE, warning = FALSE}
calc.model(
  method = "monthly",
  timespan = c(2017),
  climresp = c(5),
  classifier = c(
    "rf", 
    "pls", 
    "nnet", 
    "lm"
    ),
  seed = 707,
  p = 0.8,
  folds = "LLO",
  mnote = "vignette",
  predrows = c(8:24),
  tc_method = "cv",
  metric = "RMSE",
  autocorrelation = TRUE,
  doParallel = FALSE)
```

## Predictions

Further we can predict the scenes using the models with the `climpred` function. `climpred` also calculates an AOA.\ 

Note: There are no options yet, but it works. Because the evaluation data frame doesn't work right now, it predicts all models. In Future, it will only predict the models which perform best. 

```{r predict, eval = FALSE}
climpred(
  method = "monthly",
  mnote = "vignette", 
  AOA = FALSE)
```


Lets show the list of predictions:
```{r list predictions}
predlist <- list.files(envrmt$path_predictions, pattern = ".tif")
head(predlist)
```



```{r plot predictions}
climplot(
  mnote = "vignette",
  sensor = "Ta_200",
  aoa = FALSE,
  mapcolors = rev(heat.colors(50)),
  scale_position = "bottomleft",
  north_position = "topright"
)
```

## Validations

We can now validate the prediction with our test-dataset, that was created during the `calc.model`-function. This way, one can receive simple statistics like a cross table or a prediction summary.

Note: Validation doesn't work fully yet, because the evaluation data frame is still buggy. 
```{r validate, eval = FALSE}
validate()
```

# Plotting

To-Do:
..the code Seda made.. (Should be included as function for plotting the modelled results as maps)\
- quick map style (nice plot, and good color scale)\
- first start for the users costumized\
- tidyterra package ?\
